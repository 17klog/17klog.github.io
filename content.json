{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[{"title":"关于","date":"2019-11-13T08:59:47.829Z","updated":"2019-11-13T08:59:47.829Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"大家好，我是XXX。欢迎来到我的个人技术博客。 这里用markdown写下你的简介，就跟平时写md一样就可以了。"},{"title":"读书","date":"2019-11-13T08:59:47.829Z","updated":"2019-11-13T08:59:47.829Z","comments":true,"path":"reading/index.html","permalink":"http://yoursite.com/reading/index.html","excerpt":"","text":""}],"posts":[{"title":"CentOS7安装hadoop-1.2.1","slug":"CentOS7安装hadoop-1.2.1","date":"2019-11-13T08:26:48.760Z","updated":"2019-11-13T08:26:48.757Z","comments":true,"path":"2019/11/13/CentOS7安装hadoop-1.2.1/","link":"","permalink":"http://yoursite.com/2019/11/13/CentOS7%E5%AE%89%E8%A3%85hadoop-1.2.1/","excerpt":"","text":"CentOS7安装hadoop-1.2.1分布式环境： master slave1 slave2 centos7 centos7 centos7 1c 1c 1c 2G 1G 1G 192.168.18.30 192.168.18.31 192.168.18.32 0.初始化三个节点配置环境变量 更改主机名分别为master、slave1、slave2 配置ssh秘钥验证，达到任一节点无需密码验证登录其他站点root用户的效果 配置/etc/hosts文件，使任一节点通过master、slave1、slave2即可域名解析到对应节点IP 关闭防火墙 12345678910[root@master ~]# vim /etc/selinux/config...SELINUX=disabled...[root@master ~]# setenforce 0[root@master ~]# getenforcePermissive[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld另外从节点也要执行 1.安装java下载hadoop和jdk安装包到/usr/local/src目录下 https://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1-bin.tar.gzhttps://repo.huaweicloud.com/java/jdk/6u45-b06/jdk-6u45-linux-x64.bin 12[root@master src]# lshadoop-1.2.1-bin.tar.gz jdk-6u45-linux-x64.bin 给予jdk-6u45-linux-x64.bin可执行权限，执行如下命令： 1[root@master src]# ./jdk-6u45-linux-x64.bin 可得如下目录： 12[root@master src]# ls -ld jdk1.6.0_45drwxr-xr-x. 8 root root 176 3月 27 2013 jdk1.6.0_45 将该目录移动到/usr/local下（个人习惯）： 12[root@master jdk1.6.0_45]# pwd/usr/local/jdk1.6.0_45 此目录即为java的安装目录，现在需要将java的相关变量声明命令添加到对于脚本中。在此选择添加到/etc/bashrc文件中。 12345[root@master jdk1.6.0_45]# tail -n 5 /etc/bashrcexport JAVA_HOME=/usr/local/jdk1.6.0_45export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/libexport PATH=$PATH:$JAVA_HOME/bin 1234567[root@master jdk1.6.0_45]# source /etc/bashrc[root@master jdk1.6.0_45]# which java/usr/local/jdk1.6.0_45/bin/java[root@master jdk1.6.0_45]# java -versionjava version \"1.6.0_45\"Java(TM) SE Runtime Environment (build 1.6.0_45-b06)Java HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode) 如法炮制，在slave1和slave2节点上安装java 12[root@slave1 ~]# which java/usr/local/jdk1.6.0_45/bin/java 12[root@slave2 ~]# which java/usr/local/jdk1.6.0_45/bin/java 2.安装hadoop解压安装包 12345[root@master src]# lshadoop-1.2.1-bin.tar.gz jdk-6u45-linux-x64.bin[root@master src]# tar zxf hadoop-1.2.1-bin.tar.gz[root@master src]# lshadoop-1.2.1 hadoop-1.2.1-bin.tar.gz jdk-6u45-linux-x64.bin 移动新生成的目录到/usr/local/下 12[root@master hadoop-1.2.1]# pwd/usr/local/hadoop-1.2.1 生成存放运行过程中临时文件的tmp目录 1[root@master hadoop-1.2.1]# mkdir tmp 进入conf目录下，修改配置文件： 12[root@master conf]# pwd/usr/local/hadoop-1.2.1/conf 修改masters文件，写入master节点的自定义域名 12[root@master conf]# cat mastersmaster 修改slaves文件，写入slave节点的自定义域名 123[root@master conf]# cat slavesslave1slave2 修改core-site.xml文件 1234567891011121314[root@master conf]# vim core-site.xml...&lt;configuration&gt; &lt;!--用来指定使用hadoop时产生文件的存放目录--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop-1.2.1/tmp&lt;/value&gt; &lt;/property&gt; &lt;!--指定namenode的地址--&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml文件 123456789[root@master conf]# vim mapred-site.xml...&lt;configuration&gt; &lt;!--作业跟踪管理器的HTTP服务器访问端口和地址--&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml文件 123456789[root@master conf]# vim hdfs-site.xml...&lt;configuration&gt; &lt;!--指定hdfs保存数据的副本数量--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hadoop-env.sh脚本 123[root@master conf]# tail -n2 hadoop-env.shexport JAVA_HOME=/usr/local/jdk1.6.0_45 将/usr/local/hadoop-1.2.1目录拷贝到另外两个从节点： 12[root@master hadoop-1.2.1]# rsync -azvP /usr/local/hadoop-1.2.1 root@slave1:/usr/local/[root@master hadoop-1.2.1]# rsync -azvP /usr/local/hadoop-1.2.1 root@slave2:/usr/local/ 3.启动集群初始化 123[root@master bin]# pwd/usr/local/hadoop-1.2.1/bin[root@master bin]# ./hadoop namenode -format 启动 123456[root@master bin]# ./start-all.sh[root@master bin]# jps13144 JobTracker12906 NameNode13254 Jps13067 SecondaryNameNode 尝试执行 1234567891011121314[root@master bin]# ./hadoop fs -ls /Found 1 itemsdrwxr-xr-x - root supergroup 0 2019-02-06 14:51 /usr[root@master bin]# ./hadoop fs -put /etc/passwd /[root@master bin]# ./hadoop fs -ls /Found 2 items-rw-r--r-- 3 root supergroup 892 2019-02-06 14:53 /passwddrwxr-xr-x - root supergroup 0 2019-02-06 14:51 /usr[root@master bin]# ./hadoop fs -cat /passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologin...","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-11-12T13:07:41.834Z","updated":"2019-11-12T13:07:41.834Z","comments":true,"path":"2019/11/12/hello-world/","link":"","permalink":"http://yoursite.com/2019/11/12/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}